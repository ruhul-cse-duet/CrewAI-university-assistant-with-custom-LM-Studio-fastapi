# ğŸ“ University AI Assistant - LangChain Free Edition

> **CrewAI-powered university assistant with custom LM Studio integration**
> **No LangChain OpenAI dependency required!**

---

## âœ¨ Features

- âœ… **LangChain-free architecture** - Direct LM Studio integration
- âœ… **Multi-agent system** - Notice, Faculty, Library, Events, Department agents
- âœ… **Hybrid search** - Semantic + keyword search with FAISS
- âœ… **Bilingual support** - Bengali + English (HuggingFace multilingual embeddings)
- âœ… **Smart web scraping** - One-time scraping per question for faster responses
- âœ… **Custom tool system** - Flexible and lightweight
- âœ… **FastAPI backend** - Modern REST API
- âœ… **Streamlit frontend** - Beautiful, responsive UI
- âœ… **Minimal configuration** - Only domain name and base URL required

---

## ğŸš€ Quick Start

### Prerequisites
- Python 3.10+
- LM Studio (running on port 1234)
- 8GB RAM minimum

### Installation

```bash
# 1. Clone repository
git clone <your-repo>
cd university-ai-assistant-crewai-fastapi-lmstudio

# 2. Create virtual environment
python -m venv venv
venv\Scripts\activate  # Windows
# source venv/bin/activate  # Linux/Mac

# 3. Install dependencies
pip install -r requirements.txt

# 4. Configure environment
cp .env.example .env
# Edit .env with your settings

# 5. Run tests
python test_langchain_free.py
```

---

## ğŸ“‹ Configuration

Create `.env` file with **minimal required settings**:

```env
# REQUIRED: University Domain Name
UNIVERSITY_DOMAIN=duet.ac.bd

# REQUIRED: LM Studio Base URL (local LLM server)
LM_STUDIO_BASE_URL=http://localhost:1234/v1

# Optional: LM Studio Model Name (if not set, will use default)
# LM_STUDIO_MODEL=

# Optional: Search API Keys (if not set, will use DuckDuckGo fallback)
# SERPER_API_KEY=your_key_here
# GOOGLE_SEARCH_API_KEY=your_google_api_key
# GOOGLE_SEARCH_ENGINE_ID=your_google_cx_id
```

**Note:** The system automatically:
- Uses HuggingFace multilingual sentence transformer for Bengali/English support
- Extracts university name from domain if not provided
- Manages all other settings automatically based on user questions

---

## ğŸƒ Running the Application

### Method 1: Separate Terminals

**Terminal 1 - Backend:**
```bash
cd backend
python app.py
```

**Terminal 2 - Frontend:**
```bash
cd frontend
streamlit run streamlit_app.py
```

### Method 2: Quick Start Script
```bash
python quick_setup.py
```

### Access
- **API**: http://localhost:8000
- **UI**: http://localhost:8501
- **Docs**: http://localhost:8000/docs

---

## ğŸ“ Project Structure

```
university-ai-assistant/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ llm.py              # Custom LM Studio LLM
â”‚   â”œâ”€â”€ config.py           # Configuration
â”‚   â”œâ”€â”€ crew.py             # Agent orchestration
â”‚   â”œâ”€â”€ app.py              # FastAPI application
â”‚   â”œâ”€â”€ agents/             # Agent definitions
â”‚   â”‚   â”œâ”€â”€ notice_agent.py
â”‚   â”‚   â”œâ”€â”€ faculty_agent.py
â”‚   â”‚   â”œâ”€â”€ library_agent.py
â”‚   â”‚   â”œâ”€â”€ event_agent.py
â”‚   â”‚   â”œâ”€â”€ department_agent.py
â”‚   â”‚   â””â”€â”€ about_agent.py
â”‚   â””â”€â”€ tools/
â”‚       â”œâ”€â”€ custom_tool.py     # Custom tool decorator
â”‚       â”œâ”€â”€ embeddings.py      # Embedding generator
â”‚       â”œâ”€â”€ hybrid_search.py   # Search pipeline
â”‚       â”œâ”€â”€ faiss_store.py     # Vector store
â”‚       â”œâ”€â”€ scraper.py         # Web scraper
â”‚       â””â”€â”€ search_api.py      # Search API
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ streamlit_app.py    # UI interface
â”œâ”€â”€ requirements.txt         # Dependencies
â”œâ”€â”€ test_langchain_free.py  # Test suite
â”œâ”€â”€ quick_setup.py          # Setup script
â””â”€â”€ README.md              # This file
```

---

## ğŸ”§ LM Studio Setup

### 1. Download LM Studio
- Visit: https://lmstudio.ai/
- Download for your OS
- Install and launch

### 2. Load Model
- Open LM Studio
- Search for model: **Qwen 2.5 7B** or **Llama 3.2 3B**
- Download model
- Load model in chat

### 3. Start Server
- Click "Local Server" tab
- Select loaded model
- Start server on port **1234**
- Verify: `curl http://localhost:1234/v1/models`

---

## ğŸ¯ Usage Examples

### Query 1: Notice Information
```
User: "à¦†à¦œà¦•à§‡à¦° à¦¨à§‹à¦Ÿà¦¿à¦¶ à¦•à¦¿?"
Agent: Notice Agent
Response: "Today's notice includes..."
```

### Query 2: Faculty Information
```
User: "CSE department er teacher der list"
Agent: Faculty Agent
Response: "CSE department faculty members..."
```

### Query 3: Library Timing
```
User: "Library koto somoy khola thake?"
Agent: Library Agent
Response: "Library timings are..."
```

---

## ğŸ› ï¸ Architecture

### Custom LLM Integration
```python
# backend/llm.py
class LMStudioLLM:
    def invoke(self, prompt: str) -> str:
        # Direct API call to LM Studio
        response = requests.post(
            f"{self.base_url}/chat/completions",
            json={"model": self.model, "messages": [...]}
        )
        return response.json()["choices"][0]["message"]["content"]
```

### Custom Tool System
```python
# backend/tools/custom_tool.py
@tool(name="Search", description="Searches university")
def search_tool(query: str) -> str:
    # Search pipeline: Google â†’ Filter â†’ Scrape â†’ FAISS â†’ Results
    return results
```

### Agent Creation
```python
# backend/agents/notice_agent.py
from llm import get_llm
from tools.hybrid_search import university_hybrid_search

agent = Agent(
    role='Notice Specialist',
    llm=get_llm(),  # Custom LM Studio LLM
    tools=[university_hybrid_search]
)
```

---

## â“ Troubleshooting

### Issue 1: LM Studio Connection Error
```
Error: Cannot connect to LM Studio
```
**Solution:** Ensure LM Studio is running on port 1234

### Issue 2: Embedding Model Error
```
LM Studio embedding model not found
```
**Solution:** Set fallback in `.env`:
```env
EMBEDDING_PROVIDER=sentence-transformers
```

### Issue 4: Slow Response
- Use lighter models (1-3B parameters)
- Reduce `max_tokens` in config
- Enable GPU acceleration in LM Studio

---

## ğŸ“Š Performance

### System Requirements
- **Minimum**: 8GB RAM, 4 CPU cores
- **Recommended**: 16GB RAM, 6+ CPU cores, GPU

### Model Recommendations
| Use Case | Model | Size | Speed |
|----------|-------|------|-------|
| Development | Llama 3.2 1B | 1GB | Fast |
| Production | Qwen 2.5 7B | 4GB | Moderate |
| Best Quality | Llama 3.1 8B | 5GB | Slow |

---

## ğŸ”’ Security

- âœ… No external API keys required (self-hosted)
- âœ… Data stays local (LM Studio runs locally)
- âœ… Custom tool validation
- âš ï¸  Add CORS configuration for production

---

## ğŸš¢ Deployment

### Docker (Recommended)
```bash
docker-compose up -d
```

### Manual Deployment
1. Set up LM Studio on server
2. Configure `.env` with production settings
3. Run backend: `python backend/app.py`
4. Run frontend: `streamlit run frontend/streamlit_app.py`

---

## ğŸ“š Documentation

- **CrewAI**: https://docs.crewai.com/
- **LM Studio**: https://lmstudio.ai/docs
- **FastAPI**: https://fastapi.tiangolo.com/
- **Streamlit**: https://docs.streamlit.io/

---

## ğŸ¤ Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create feature branch
3. Test thoroughly
4. Submit pull request

---

## ğŸ“ License

MIT License - See LICENSE file

---

## ğŸ‰ Benefits of LangChain-Free Approach

### Why We Removed LangChain OpenAI:
1. **Lighter dependencies** - Faster installation
2. **Better control** - Direct API integration
3. **No API costs** - Fully self-hosted
4. **Easier debugging** - Simpler architecture
5. **More flexible** - Custom implementations

### Performance Comparison:
| Metric | With LangChain | Without LangChain |
|--------|----------------|-------------------|
| Install time | ~5 min | ~2 min |
| Memory usage | ~800MB | ~400MB |
| Dependencies | 50+ | 20+ |
| Startup time | ~10s | ~3s |

---

## ğŸ“ Support

- **Issues**: GitHub Issues
- **Docs**: `/docs` folder
- **Tests**: `python test_langchain_free.py`

---

**Made with â¤ï¸ for university students**
